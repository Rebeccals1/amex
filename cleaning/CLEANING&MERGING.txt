Overview of Cleaning and Merging Each Dataset


USERS
* Dropped users that weren’t located in the United States
   * (country didn’t equal United States)
* Dropped the country column since all values were United States
* Dropped columns that were too specific
   * First_name, last_name, email, street_address, latitude, longitude, created_at
*  Dropped the city column
   * 170 instances were missing from here, and over 2000 were missing from events. It is likely highly correlated with zip code, and we would have to convert it to numerical anyways
* ‘Gender’ was dropped because the ‘orders’ dataframe also contains gender, and the information would be redundant upon merging
PRODUCTS
* Dropped ‘distribution_center_id’ and ‘sku’
* Only columns with outliers were ‘cost’ and ‘retail_price’
   * Some of the columns had outliers that made sense (ex. Brand ‘moncler’ had an average price of $550, so a product costing $575 wasn’t a concern
   * However others, such as brand ‘asics’ had an average price of around $50, so having a product costing $600 was definitely a mistake)
   * Even the valid prices for brands would still skew the distribution, so values in ‘retail_price’ and ‘cost’ that were above the 95th percentile were removed.
      * There is potential to build a function that would remove prices/costs that are above a certain threshold of the average for the brand. However, for now outliers above the 95th percentile were removed
      * Cost threshold: 83
      * Retail_price threshold: 170
* Dropped instances with na values: 24 missing brand values, 2 missing name values
ORDERS
* Converted ‘status’ column to dummy variables (canceled, complete, processing, returned, shipped)
* Dropped columns that are now represented by dummy variables
   * * Tried to convert ‘created_at’  to extract just month and year but it introduced na values, so it was left in datetime format and can be dealt with later.
   * Ended up dropping ‘created_at’ column, since it needed to be kept in the ‘order_items’ dataframe for merging purposes and it would’ve been repetitive.
ORDER_ITEMS
* Converted ‘status’ column to dummy variables, as with orders
   * Upon further analysis, it was found that these columns were redundant (the status for an item in an order was the same as the status for the order) so these columns were later dropped
* ‘Sale_price’ was dropped because it was equivalent to ‘retail_price’ from the products column
* ‘Inventory_item_id’ was dropped because it isn’t needed for merging
* ‘Created_at’ was dropped because it was equivalent/very similar to ‘created_at’ from the ‘orders’ dataframe
   * ‘Created_at’ was eventually reinstated because it had to be used as a key (along with ‘user_id’) to join the ‘events’ table
   * Below is an explanation of why only events of type ‘purchase’ are going to be able to be used - however since ‘user_id’ is the only key that connects ‘events’ with other tables, we have no way of knowing which purchase event is associated with which order item if the user made multiple purchases. Therefore, ‘created_at’ will be used as a key for merging purposes. 
EVENTS
* Dropped instances where ‘user_id’ was null (guest browsing)
* Dropped ‘session_id’, ‘ip_address’, ‘uri’ and ‘city’ because they were too specific
   * Reason for dropping the city column was mentioned in ‘users’


ISSUE CONCERNING MERGING EVENTS: 
  

Above shows all of the events for one user (user_id==46). User 46 made 2 purchases, in 2 orders, at 2 different times.
Order_items for user 46 v
  

In event_type, an event is created when a user adds a product to cart, navigates to a department, makes a purchase, clicks on a product or returns home. 
‘Events’ only contains the id for the event and the id for the user. Therefore, we don’t have a way of knowing which product was added to cart.
If we try to merge events with the rest of the merged dataframe, each event will be represented by an order item. However, only events of type ‘purchase’ should be associated with an order item. 
This means that realistically, only events of type ‘purchase’ are going to be able to be used for merging purposes.
However, the rest of the data in events is still valuable. 
* Created_at: If you look at the time stamps, you can see that the events align with the times the separate order was placed. This time stamp can also be used to tell the time from when a user began browsing, to when they added the item to cart, to when they purchased the item. 
* Sequence_number: This tells the sequence of events the user took leading up to the purchase. 1 is either home or department, while 5 is usually purchase. 
These portions will have to be lost during merging, unless the rest of the data about the order is null (which won’t be able to be used for modeling purposes).


* The only way to merge is to only keep events of type ‘purchase’, so that’s what was done. 


MERGING KEYS:
* As stated above, only events of type ‘purchase’ were kept. 
* Upon further analysis, when trying to merge the ‘events’ table with the other table (which consisted of users, orders, order_items, and products) an issue was encountered.
   * The only key included in ‘events’ that matches ‘orders_users_products’ is ‘user_id’
   * This is an issue because, if a user placed multiple orders or had one order with multiple items, we have no way of knowing which purchase event is associated with which order item. The information from ‘events’ only tells us which user conducted the event. 
* Therefore, another unique key from ‘events’ is needed to connect the event to the item.
   * The only other possible unique key that could be used was ‘created_at’, which (upon analysis) seemed to match up to the ‘created_at’ column from ‘order_items’
      * This can introduce another issue, however, because this column is in datetime format.
      * If each column is not perfectly formatted/doesn’t exactly match in both tables, rows are dropped from the dataframe. 
CONCLUSION:
* Before merging, the sizes of each table were as follows
   * Orders_users_products: 38,152
   * US_events (of event_type ‘purchase’): 40,513
* After merging, all_data was of size 19,696
   * This confirmed suspicions of information loss from using the ‘created_at’ column as a key. However, this final dataframe is formatted correctly.


POSSIBLE SOLUTIONS:
* We can consult with the challenge advisor/TA on how to properly merge. 
* We can do further analysis to see which rows were dropped and why, perhaps fix the format of both created_at columns
* (This may be the best option) We can look at other tables that were included in the data and see if we can merge one with events that can give us another unique key that aligns with orders_users_products.